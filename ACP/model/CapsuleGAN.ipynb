{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Current directory\n",
    "import os\n",
    "os.chdir('F:/One/ACP')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "# train\n",
    "train_features = pd.read_csv(\"features_label/train.csv\", index_col=False, header=None)\n",
    "train_labels = pd.read_csv(\"features_label/train_label.csv\", index_col=False, header=None)\n",
    "# test\n",
    "test_features = pd.read_csv(\"features_label/test.csv\", index_col=False, header=None)\n",
    "test_labels = pd.read_csv(\"features_label/test_label.csv\", index_col=False, header=None)\n",
    "\n",
    "print('train_features:', train_features.shape)\n",
    "print('train_labels', train_labels.shape)\n",
    "print('test_features', test_features.shape)\n",
    "print('test_labels', test_labels.shape)\n",
    "\n",
    "\n",
    "train_features = np.array(train_features)\n",
    "train_labels = np.array(train_labels)\n",
    "test_features = np.array(test_features)\n",
    "test_labels = np.array(test_labels)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statistics\n",
    "import os,sys,math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from keras import backend as K\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.optimizers import adam_v2\n",
    "from keras.activations import sigmoid\n",
    "from sklearn.preprocessing import scale\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from keras.models import Sequential, Model\n",
    "from keras import layers, models, optimizers\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from keras.layers import multiply, Add, Permute\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "from sklearn.metrics import average_precision_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "from keras.layers import BatchNormalization, Activation\n",
    "from keras import initializers, regularizers, constraints\n",
    "from keras.layers.convolutional import UpSampling2D, Conv2D\n",
    "from keras.layers import Input, Dense, Layer, Reshape, Flatten\n",
    "from keras.layers import Dropout, Lambda, Concatenate, Multiply\n",
    "from keras.layers import GlobalAveragePooling2D, GlobalMaxPooling2D\n",
    "\n",
    "\n",
    "def get_shuffle(data,label):    \n",
    "    #shuffle data\n",
    "    index = [i for i in range(len(label))]\n",
    "    np.random.shuffle(index)\n",
    "    data = data[index]\n",
    "    label = label[index]\n",
    "    return data,label \n",
    "\n",
    "\n",
    "def scale_mean_var(input_arr,axis=0):\n",
    "    #from sklearn import preprocessing\n",
    "    #input_arr= preprocessing.scale(input_arr.astype('float'))\n",
    "    mean_ = np.mean(input_arr,axis=0)\n",
    "    scale_ = np.std(input_arr,axis=0)\n",
    "    # 减均值 \n",
    "    output_arr= input_arr- mean_\n",
    "    # 判断均值是否接近0\n",
    "    mean_1 = output_arr.mean(axis=0)\n",
    "    if not np.allclose(mean_1, 0):\n",
    "        output_arr -= mean_1\n",
    "    # 将标准差为0元素的置1\n",
    "    #scale_ = _handle_zeros_in_scale(scale_, copy=False)\n",
    "    scale_[scale_ == 0.0] = 1.0\n",
    "    # 除以标准差\n",
    "    output_arr /=scale_\n",
    "    # 再次判断均值是否为0\n",
    "    mean_2 = output_arr .mean(axis=0)\n",
    "    if not np.allclose(mean_2, 0):\n",
    "        output_arr  -= mean_2\n",
    "\n",
    "    return output_arr\n",
    "\n",
    "\n",
    "########################################################### Def Cbam\n",
    "def channel_attention(input_feature, ratio=8):\n",
    "\tchannel_axis = 1 if K.image_data_format() == \"channels_first\" else -1\n",
    "\tchannel = input_feature.shape[channel_axis]\n",
    "\tshared_layer_one = Dense(channel//ratio,\n",
    "\t\t\t\t\t\t\t kernel_initializer='he_normal',\n",
    "\t\t\t\t\t\t\t activation = 'relu',\n",
    "\t\t\t\t\t\t\t use_bias=True,\n",
    "\t\t\t\t\t\t\t bias_initializer='zeros')\n",
    "\tshared_layer_two = Dense(channel,\n",
    "\t\t\t\t\t\t\t kernel_initializer='he_normal',\n",
    "\t\t\t\t\t\t\t use_bias=True,\n",
    "\t\t\t\t\t\t\t bias_initializer='zeros')\n",
    "\tavg_pool = GlobalAveragePooling2D()(input_feature)    \n",
    "\tavg_pool = Reshape((1,1,channel))(avg_pool)\n",
    "\tassert avg_pool.shape[1:] == (1,1,channel)\n",
    "\tavg_pool = shared_layer_one(avg_pool)\n",
    "\tassert avg_pool.shape[1:] == (1,1,channel//ratio)\n",
    "\tavg_pool = shared_layer_two(avg_pool)\n",
    "\tassert avg_pool.shape[1:] == (1,1,channel)\n",
    "\tmax_pool = GlobalMaxPooling2D()(input_feature)\n",
    "\tmax_pool = Reshape((1,1,channel))(max_pool)\n",
    "\tassert max_pool.shape[1:] == (1,1,channel)\n",
    "\tmax_pool = shared_layer_one(max_pool)\n",
    "\tassert max_pool.shape[1:] == (1,1,channel//ratio)\n",
    "\tmax_pool = shared_layer_two(max_pool)\n",
    "\tassert max_pool.shape[1:] == (1,1,channel)\n",
    "\tcbam_feature = Add()([avg_pool,max_pool])\n",
    "\tcbam_feature = Activation('hard_sigmoid')(cbam_feature)\n",
    "\tif K.image_data_format() == \"channels_first\":\n",
    "\t\tcbam_feature = Permute((3, 1, 2))(cbam_feature)\n",
    "\treturn multiply([input_feature, cbam_feature])\n",
    "\n",
    "\n",
    "def spatial_attention(input_feature):\n",
    "\tkernel_size = 7\n",
    "\tif K.image_data_format() == \"channels_first\":\n",
    "\t\tchannel = input_feature.shape[1]\n",
    "\t\tcbam_feature = Permute((2,3,1))(input_feature)\n",
    "\telse:\n",
    "\t\tchannel = input_feature.shape[-1]\n",
    "\t\tcbam_feature = input_feature\n",
    "\tavg_pool = Lambda(lambda x: K.mean(x, axis=3, keepdims=True))(cbam_feature)\n",
    "\tassert avg_pool.shape[-1] == 1\n",
    "\tmax_pool = Lambda(lambda x: K.max(x, axis=3, keepdims=True))(cbam_feature)\n",
    "\tassert max_pool.shape[-1] == 1\n",
    "\tconcat = Concatenate(axis=3)([avg_pool, max_pool])\n",
    "\tassert concat.shape[-1] == 2\n",
    "\tcbam_feature = Conv2D(filters = 1,\n",
    "\t\t\t\t\tkernel_size=kernel_size,\n",
    "\t\t\t\t\tactivation = 'hard_sigmoid',\n",
    "\t\t\t\t\tstrides=1,\n",
    "\t\t\t\t\tpadding='same',\n",
    "\t\t\t\t\tkernel_initializer='he_normal',\n",
    "\t\t\t\t\tuse_bias=False)(concat)\n",
    "\tassert cbam_feature.shape[-1] == 1\n",
    "\tif K.image_data_format() == \"channels_first\":\n",
    "\t\tcbam_feature = Permute((3, 1, 2))(cbam_feature)\n",
    "\treturn multiply([input_feature, cbam_feature])\n",
    "\n",
    "\n",
    "def cbam_block(cbam_feature,ratio=8):\n",
    "\tcbam_feature = channel_attention(cbam_feature, ratio)\n",
    "\tcbam_feature = spatial_attention(cbam_feature, )\n",
    "\treturn cbam_feature\n",
    "\n",
    "\n",
    "############################################## Def discriminator and generator\n",
    "def squash(vectors, axis=-1):\n",
    "    s_squared_norm = K.sum(K.square(vectors), axis, keepdims=True)\n",
    "    scale = s_squared_norm / (1 + s_squared_norm) / K.sqrt(s_squared_norm + K.epsilon())\n",
    "    return scale * vectors\n",
    "\n",
    "\n",
    "def build_discriminator():\n",
    "    img = Input(shape=(1,input_dimwx,1))\n",
    "    x = Conv2D(filters=64, kernel_size=(1,9), strides=2, padding='valid', name='conv1')(img)\n",
    "    x = LeakyReLU()(x)\n",
    "    x = BatchNormalization(momentum=0.8)(x)\n",
    "    \n",
    "    x = Conv2D(filters=32, kernel_size=(1,9), strides=2, padding='valid', name='conv1')(img)\n",
    "    x = LeakyReLU()(x)\n",
    "    x = BatchNormalization(momentum=0.8)(x)\n",
    "    \n",
    "    \"\"\"\n",
    "    NOTE: Capsule architecture starts from here.\n",
    "    \"\"\"\n",
    "    ##### primarycaps coming first ##### \n",
    "    x = Conv2D(filters=32, kernel_size=(1,3), strides=2, padding='valid', name='primarycap_conv2')(x)    \n",
    "    [aa,bb,cc,dd] = x.shape\n",
    "    numx = int(cc)\n",
    "    x = Reshape(target_shape=[-1, numx], name='primarycap_reshape')(x)\n",
    "    x = Lambda(squash, name='primarycap_squash')(x)\n",
    "    x = BatchNormalization(momentum=0.8)(x)\n",
    "    \n",
    "    ##### digitcaps are here ##### \n",
    "    x = Flatten()(x)\n",
    "    uhat = Dense(128, kernel_initializer='he_normal', bias_initializer='zeros', name='uhat_digitcaps')(x)\n",
    "    c = Activation('softmax', name='softmax_digitcaps1')(uhat) # softmax will make sure that each weight c_ij is a non-negative number and their sum equals to one\n",
    "    c = Dense(128)(c) # compute s_j\n",
    "    x = Multiply()([uhat, c])\n",
    "    \"\"\"\n",
    "    NOTE: Squashing the capsule outputs creates severe blurry artifacts, thus we replace it with Leaky ReLu.\n",
    "    \"\"\"\n",
    "    s_j = LeakyReLU()(x)\n",
    "    ##### we will repeat the routing part 2 more times (num_routing=3) to unfold the loop\n",
    "    c = Activation('softmax', name='softmax_digitcaps2')(s_j) # softmax will make sure that each weight c_ij is a non-negative number and their sum equals to one\n",
    "    c = Dense(128)(c) # compute s_j\n",
    "    x = Multiply()([uhat, c])\n",
    "    s_j = LeakyReLU()(x)\n",
    "\n",
    "    c = Activation('softmax', name='softmax_digitcaps3')(s_j) # softmax will make sure that each weight c_ij is a non-negative number and their sum equals to one\n",
    "    c = Dense(128)(c) # compute s_j\n",
    "    x = Multiply()([uhat, c])\n",
    "    s_j = LeakyReLU()(x)\n",
    "    \n",
    "    c = Activation('softmax', name='softmax_digitcaps4')(s_j) # softmax will make sure that each weight c_ij is a non-negative number and their sum equals to one\n",
    "    c = Dense(128)(c) # compute s_j\n",
    "    x = Multiply()([uhat, c])\n",
    "    s_j = LeakyReLU()(x)\n",
    "    # ##### preparition for cbam_block\n",
    "    s_j = Reshape((-1,128,1))(s_j)\n",
    "    inputs = s_j\n",
    "    residual = Conv2D(filters=64, kernel_size=(1,1), strides=1, padding='same', name='convxxx')(inputs)\n",
    "    residual = BatchNormalization(momentum=0.8)(residual)\n",
    "    \n",
    "    cbam = cbam_block(residual)\n",
    "    # cbam = channel_attention(residual)\n",
    "    # cbam = spatial_attention(residual)\n",
    "    \n",
    "    cbam = Reshape((-1,))(cbam)\n",
    "    pred = Dense(2, activation='sigmoid')(cbam)\n",
    "    \n",
    "    # cbam = Reshape((-1,))(s_j)\n",
    "    # pred = Dense(2, activation='sigmoid')(cbam)\n",
    "    \n",
    "    \n",
    "    return Model(img, pred)\n",
    "\n",
    "\n",
    "# generator structure\n",
    "def build_generator():\n",
    "    \"\"\"\n",
    "    Generator follows the DCGAN architecture and creates generated image representations through learning.\n",
    "    \"\"\"\n",
    "    noise_shape =(input_dimwx,)\n",
    "    x_noise = Input(shape=noise_shape)\n",
    "    # we apply different kernel sizes in order to match the original image size\n",
    "    x = Dense(64 * 1 * input_dimwx, activation=\"relu\")(x_noise)\n",
    "    x = Reshape((1, input_dimwx, 64))(x)\n",
    "    x = BatchNormalization(momentum=0.2)(x)\n",
    "    x = UpSampling2D()(x)\n",
    "    [aa1,bb1,cc1,dd1] = x.shape\n",
    "    numx1 = int(cc1//4)\n",
    "    x = Conv2D(32, kernel_size=(2,numx1), padding=\"valid\")(x)\n",
    "    x = Activation(\"relu\")(x)\n",
    "    x = BatchNormalization(momentum=0.2)(x)\n",
    "    [aa2,bb2,cc2,dd2] = x.shape\n",
    "    #### x = UpSampling2D()(x)\n",
    "    numx2 = int(1+cc2-input_dimwx)\n",
    "    x = Conv2D(16, kernel_size=(1,numx2), padding=\"valid\")(x)\n",
    "    x = Activation(\"relu\")(x)\n",
    "    x = BatchNormalization(momentum=0.2)(x)\n",
    "    x = Conv2D(1, kernel_size=3, padding=\"same\")(x)\n",
    "    gen_out = Activation(\"tanh\")(x)\n",
    "        \n",
    "    return Model(x_noise, gen_out)\n",
    "\n",
    "\n",
    "def categorical_probas_to_classes(p):\n",
    "    return np.argmax(p, axis=1)\n",
    "\n",
    "\n",
    "def to_categorical(y, nb_classes=None):\n",
    "    y = np.array(y, dtype='int')\n",
    "    if not nb_classes:\n",
    "        nb_classes = np.max(y)+1\n",
    "    Y = np.zeros((len(y), nb_classes))\n",
    "    for i in range(len(y)):\n",
    "        Y[i, y[i]] = 1\n",
    "    return Y\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training and prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Independent testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a directory to save the model\n",
    "model_save_dir = 'save_models/CapsuleGAN/Independence'\n",
    "os.makedirs(model_save_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_set = scale(X_new)\n",
    "[sample_num, input_dimwx] = np.shape(train_features)\n",
    "X_train = train_features\n",
    "y_train = train_labels\n",
    "X_test = test_features\n",
    "y_test = test_labels\n",
    "\n",
    "ACC_collecton = []\n",
    "BACC_collecton = []\n",
    "Sn_collecton = []\n",
    "Sp_collecton = []\n",
    "MCC_collecton = []\n",
    "AUC_collecton = []\n",
    "AP=[]\n",
    "\n",
    "mean_recall = np.linspace(0, 1, 100)\n",
    "all_precision = []\n",
    "base_fpr = np.linspace(0, 1, 100)\n",
    "mean_tpr = 0.0\n",
    "interp_tpr_collection = []\n",
    "\n",
    "losses = []  # 用于存储每个fold的损失值\n",
    "for i in range(10):\n",
    "    # Loading the model\n",
    "    discriminator = build_discriminator()\n",
    "    discriminator.compile(loss='binary_crossentropy', optimizer=adam_v2.Adam(0.001), metrics=['accuracy'])\n",
    "\n",
    "    generator = build_generator()\n",
    "    generator.compile(loss='binary_crossentropy', optimizer=adam_v2.Adam(0.001), metrics=['accuracy'])\n",
    "\n",
    "    # feeding noise to generator\n",
    "    z = Input(shape=(input_dimwx,))\n",
    "    img = generator(z)\n",
    "    # for the combined model we will only train the generator\n",
    "    discriminator.trainable = False\n",
    "    # try to discriminate generated images\n",
    "    valid = discriminator(img)\n",
    "    # the combined model (stacked generator and discriminator) takes\n",
    "    # noise as input => generates images => determines validity \n",
    "    combined = Model(z, valid)\n",
    "    combined.compile(loss='binary_crossentropy', optimizer=adam_v2.Adam(0.001), metrics=['accuracy'])\n",
    "    \n",
    "    # dataset\n",
    "    y_train_whole = to_categorical(y_train)\n",
    "    cv_clf = combined\n",
    "    hist = cv_clf.fit(X_train, y_train_whole, batch_size=64, epochs=90)\n",
    "\n",
    "    losses.append(hist.history['loss'])  # 记录每个epoch的损失值\n",
    "\n",
    "    y_score = cv_clf.predict(X_test)\n",
    "    y_class = categorical_probas_to_classes(y_score)\n",
    "    TP, FP, FN, TN = confusion_matrix(y_test, y_class).ravel() # shape [ [True-Positive, False-positive], [False-negative, True-negative] ]\n",
    "    Sn_collecton.append(TP/(TP+FN))\n",
    "    Sp_collecton.append(TN/(TN+FP))\n",
    "    MCC = (TP*TN-FP*FN)/math.pow(((TP+FP)*(TP+FN)*(TN+FP)*(TN+FN)),0.5)\n",
    "    MCC_collecton.append(MCC)\n",
    "    BACC_collecton.append(0.5*TP/(TP+FN)+0.5*TN/(TN+FP))\n",
    "    ACC = (TP+TN)/(TP+TN+FP+FN)\n",
    "    ACC_collecton.append(ACC)\n",
    "    # ROC curve\n",
    "    fpr, tpr, _ = roc_curve(y_test, y_score[:, 1])\n",
    "    interp_tpr = np.interp(base_fpr, fpr, tpr)\n",
    "    interp_tpr[0] = 0.0\n",
    "    interp_tpr_collection.append(interp_tpr)\n",
    "    auc_roc = auc(fpr, tpr)\n",
    "    AUC_collecton.append(auc_roc)\n",
    "    # PR curve\n",
    "    precision, recall, _ = precision_recall_curve(y_test, y_score[:, 1])\n",
    "    average_precision = average_precision_score(y_test, y_score[:, 1])\n",
    "    recall = np.flipud(recall)\n",
    "    precision = np.flipud(precision)\n",
    "\n",
    "    mean_precision = np.interp(mean_recall, recall, precision)\n",
    "    all_precision.append(mean_precision)\n",
    "    AP.append(average_precision)\n",
    "\n",
    "# 在所有交叉验证循环结束后,计算TPR的均值\n",
    "mean_tpr = np.mean(interp_tpr_collection, axis=0)\n",
    "mean_tpr[-1] = 1.0\n",
    "\n",
    "# Calculate the mean precision\n",
    "mean_precision = np.mean(all_precision, axis=0)\n",
    "\n",
    "# Output\n",
    "results = [\n",
    "    f\"ACC: {round(statistics.mean(ACC_collecton), 3)} ± {round(statistics.stdev(ACC_collecton), 3)}\",\n",
    "    f\"BACC: {round(statistics.mean(BACC_collecton), 3)} ± {round(statistics.stdev(BACC_collecton), 3)}\",\n",
    "    f\"Sn: {round(statistics.mean(Sn_collecton), 3)} ± {round(statistics.stdev(Sn_collecton), 3)}\",\n",
    "    f\"Sp: {round(statistics.mean(Sp_collecton), 3)} ± {round(statistics.stdev(Sp_collecton), 3)}\",\n",
    "    f\"MCC: {round(statistics.mean(MCC_collecton), 3)} ± {round(statistics.stdev(MCC_collecton), 3)}\",\n",
    "    f\"AUC: {round(statistics.mean(AUC_collecton), 3)} ± {round(statistics.stdev(AUC_collecton), 3)}\",\n",
    "    f\"AP: {round(statistics.mean(AP), 3)} ± {round(statistics.stdev(AP), 3)}\"\n",
    "]\n",
    "\n",
    "for result in results:\n",
    "    print(result)\n",
    "\n",
    "# Append the results to the file\n",
    "with open('result/results_CapsuleGAN.txt', 'a') as file:\n",
    "    file.write(\"----------------------------------------\\n\")\n",
    "    for result in results:\n",
    "        file.write(result + '\\n')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 绘制损失函数曲线\n",
    "from matplotlib.ticker import FuncFormatter\n",
    "\n",
    "n = 0\n",
    "loss = losses[n]\n",
    "plt.figure()\n",
    "\n",
    "plt.plot(loss)\n",
    "# for i, loss in enumerate(losses):\n",
    "#     plt.plot(loss, label=f'Fold {i+1}')\n",
    "\n",
    "formatter = FuncFormatter(lambda x, pos: f'{x:.2f}')\n",
    "plt.gca().yaxis.set_major_formatter(formatter)\n",
    "\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Loss Curve')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "# plt.savefig('loss_curve.png', dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 绘制ROC曲线\n",
    "plt.figure()\n",
    "lw = 2\n",
    "plt.plot(base_fpr, mean_tpr, color='darkorange', lw=lw, label='ROC curve (area = %0.2f)' % np.mean(AUC_collecton))\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\n",
    "plt.xlim([-0.05, 1.05])\n",
    "plt.ylim([-0.05, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Independence test')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.tight_layout()\n",
    "# plt.savefig('loss_curve.png', dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### end"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
