{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Current directory\n",
    "import os\n",
    "os.chdir('F:/One/ACP')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ESM feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def esm_embeddings(peptide_sequence_list):\n",
    "  # NOTICE: ESM for embeddings is quite RAM usage, if your sequence is too long, \n",
    "  #         or you have too many sequences for transformation in a single converting, \n",
    "  #         you conputer might automatically kill the job.\n",
    "  import torch\n",
    "  import esm\n",
    "  import pandas as pd\n",
    "  import collections\n",
    "  # load the model\n",
    "  # NOTICE: if the model was not downloaded in your local environment, it will automatically download it.\n",
    "  model, alphabet = esm.pretrained.esm2_t6_8M_UR50D()\n",
    "  batch_converter = alphabet.get_batch_converter()\n",
    "  model.eval()  # disables dropout for deterministic results\n",
    "\n",
    "  # load the peptide sequence list into the bach_converter\n",
    "  batch_labels, batch_strs, batch_tokens = batch_converter(peptide_sequence_list)\n",
    "  batch_lens = (batch_tokens != alphabet.padding_idx).sum(1)\n",
    "  ## batch tokens are the embedding results of the whole data set\n",
    "\n",
    "  # Extract per-residue representations (on CPU)\n",
    "  with torch.no_grad():\n",
    "      # Here we export the last layer of the EMS model output as the representation of the peptides\n",
    "      # model'esm2_t6_8M_UR50D' only has 6 layers, and therefore repr_layers parameters is equal to 6\n",
    "      results = model(batch_tokens, repr_layers=[6], return_contacts=True)  \n",
    "  token_representations = results[\"representations\"][6]\n",
    "\n",
    "  # Generate per-sequence representations via averaging\n",
    "  # NOTE: token 0 is always a beginning-of-sequence token, so the first residue is token 1.\n",
    "  sequence_representations = []\n",
    "  for i, tokens_len in enumerate(batch_lens):\n",
    "      sequence_representations.append(token_representations[i, 1 : tokens_len - 1].mean(0))\n",
    "  # save dataset\n",
    "  # sequence_representations is a list and each element is a tensor\n",
    "  embeddings_results = collections.defaultdict(list)\n",
    "  for i in range(len(sequence_representations)):\n",
    "      # tensor can be transformed as numpy sequence_representations[0].numpy() or sequence_representations[0].to_list\n",
    "      each_seq_rep = sequence_representations[i].tolist()\n",
    "      for each_element in each_seq_rep:\n",
    "          embeddings_results[i].append(each_element)\n",
    "  embeddings_results = pd.DataFrame(embeddings_results).T\n",
    "  return embeddings_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 加载序列"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1378,)\n",
      "(1378,)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "# 加载训练数据集\n",
    "dataset = pd.read_csv('data/train.tsv', sep='\\t', na_filter=False)  # 读取TSV文件\n",
    "sequence_list = dataset['text_a']\n",
    "# 加载用于模型开发的y数据集\n",
    "label = dataset['label']\n",
    "\n",
    "# 保存为CSV文件\n",
    "label.to_csv('features_label/train_label.csv', index=False, header=None)\n",
    "\n",
    "print(sequence_list.shape)\n",
    "print(label.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 提取ESM-2特征"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load sequence for esm-2\n",
    "peptide_sequence_list = []\n",
    "for seq in sequence_list:\n",
    "    format_seq = [seq,seq] # the setting is just following the input format setting in ESM model, [name,sequence]\n",
    "    tuple_sequence = tuple(format_seq)\n",
    "    peptide_sequence_list.append(tuple_sequence) # build a summarize list variable including all the sequence information\n",
    "\n",
    "# employ ESM model for converting and save the converted data in csv format\n",
    "embeddings_results = esm_embeddings(peptide_sequence_list)\n",
    "embeddings_results.to_csv('features_label/train.csv', index=False, header=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PortT5 feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define function\n",
    "def PortT5_embedding(sequence_list):\n",
    "    \n",
    "    from transformers import T5Tokenizer, T5EncoderModel\n",
    "    import torch\n",
    "    import re\n",
    "\n",
    "    # Calculate the length of a sequence\n",
    "    sequence_lengths = [len(sequence) for sequence in sequence_list]\n",
    "\n",
    "    device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "    # Load the tokenizer\n",
    "    tokenizer = T5Tokenizer.from_pretrained('Rostlab/prot_t5_xl_half_uniref50-enc', do_lower_case=False)\n",
    "\n",
    "    # Load the model\n",
    "    model = T5EncoderModel.from_pretrained(\"Rostlab/prot_t5_xl_half_uniref50-enc\").to(device)\n",
    "\n",
    "    # only GPUs support half-precision currently; if you want to run on CPU use full-precision (not recommended, much slower)\n",
    "    model.full() if device=='cpu' else model.half()\n",
    "\n",
    "    # replace all rare/ambiguous amino acids by X and introduce white-space between all amino acids\n",
    "    sequence_list = [\" \".join(list(re.sub(r\"[UZOB]\", \"X\", sequence))) for sequence in sequence_list]\n",
    "\n",
    "    # tokenize sequences and pad up to the longest sequence in the batch\n",
    "    ids = tokenizer(sequence_list, add_special_tokens=True, padding=\"longest\")\n",
    "\n",
    "    input_ids = torch.tensor(ids['input_ids']).to(device)\n",
    "    attention_mask = torch.tensor(ids['attention_mask']).to(device)\n",
    "\n",
    "    # generate embeddings\n",
    "    with torch.no_grad():\n",
    "        embedding_repr = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "\n",
    "    # extract residue embeddings for each sequence in the batch and remove padded & special tokens\n",
    "    embeddings = [embedding_repr.last_hidden_state[i, :length] for i, length in enumerate(sequence_lengths)]\n",
    "\n",
    "    # if you want to derive a single representation (per-protein embedding) for the whole protein\n",
    "    per_protein_embeddings = [emb.mean(dim=0) for emb in embeddings]\n",
    "\n",
    "    return per_protein_embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 加载序列数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# training dataset loading\n",
    "dataset = pd.read_excel('1030_dataset.xlsx', na_filter = False) # take care the NA sequence problem\n",
    "sequence_list = dataset['sequences']\n",
    "# # loading the y dataset for model development \n",
    "y = dataset['label']\n",
    "y = np.array(y) # transformed as np.array for CNN model\n",
    "\n",
    "# # 验证\n",
    "print(sequence_list)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 提取PortT5特征"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load sequence for PortT5\n",
    "\n",
    "# 使用PortT5模型进行序列嵌入\n",
    "embeddings_results = PortT5_embedding(sequence_list)\n",
    "\n",
    "# 将嵌入结果的tensor转换为数值\n",
    "embeddings_results = [[float(value) for value in result] for result in embeddings_results]\n",
    "\n",
    "# 将嵌入结果列表转换为DataFrame格式\n",
    "embeddings_results = pd.DataFrame(embeddings_results)\n",
    "\n",
    "# 将DataFrame导出为CSV文件\n",
    "embeddings_results.to_csv('PortT5/PortT5_dataset.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 结尾"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
